{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "NlgwlGYptL6C"
      },
      "outputs": [],
      "source": [
        "!pip install -q google.generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "oO3zZwhTtnkZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as palm\n",
        "import os"
      ],
      "metadata": {
        "id": "EXF7hTcRt5bs"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palm.configure(api_key=\"AIzaSyDyekfj8eIz_frVmj-3BgF0oYkCLSn2bIo\")"
      ],
      "metadata": {
        "id": "Ws7iOlhyuHcC"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(palm.list_models())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcimMcGi3YFs",
        "outputId": "68c1775c-ef0d-4f34-b2ca-b692aa5a25ea"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001', base_model_id='', version='001', display_name='Chat Bison', description='Chat-optimized generative language model.', input_token_limit=4096, output_token_limit=1024, supported_generation_methods=['generateMessage', 'countMessageTokens'], temperature=0.25, top_p=0.95, top_k=40),\n",
              " Model(name='models/text-bison-001', base_model_id='', version='001', display_name='Text Bison', description='Model targeted for text generation.', input_token_limit=8196, output_token_limit=1024, supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'], temperature=0.7, top_p=0.95, top_k=40),\n",
              " Model(name='models/embedding-gecko-001', base_model_id='', version='001', display_name='Embedding Gecko', description='Obtain a distributed representation of a text.', input_token_limit=1024, output_token_limit=1, supported_generation_methods=['embedText'], temperature=None, top_p=None, top_k=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in palm.list_models():\n",
        "  print(f\"model.name: {model.name}\")\n",
        "  print(f\"model.description: {model.description}\")\n",
        "  print(f\"model.supported_generation_methods: {model.supported_generation_methods}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2xujCUXuTiZ",
        "outputId": "c3211ab2-ae16-492c-bf0d-0719c87db5d1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.name: models/chat-bison-001\n",
            "model.description: Chat-optimized generative language model.\n",
            "model.supported_generation_methods: ['generateMessage', 'countMessageTokens']\n",
            "model.name: models/text-bison-001\n",
            "model.description: Model targeted for text generation.\n",
            "model.supported_generation_methods: ['generateText', 'countTextTokens', 'createTunedTextModel']\n",
            "model.name: models/embedding-gecko-001\n",
            "model.description: Obtain a distributed representation of a text.\n",
            "model.supported_generation_methods: ['embedText']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = [model for model in palm.list_models() if 'generateText' in model.supported_generation_methods]\n",
        "models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mtfXiSYvALJ",
        "outputId": "4832d05c-efc9-4224-dc17-9556acaf3dc8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/text-bison-001', base_model_id='', version='001', display_name='Text Bison', description='Model targeted for text generation.', input_token_limit=8196, output_token_limit=1024, supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'], temperature=0.7, top_p=0.95, top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_bison = models[0]"
      ],
      "metadata": {
        "id": "q7lXUuKjvNda"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Generate_summary(text):\n",
        "  prompt = f\"\"\"\n",
        "  Your task is to act as a Text Summarizer.\n",
        "  I'll Give you text.\n",
        "  Your task is to create summary for the text in 100 words.\n",
        "  text is shared below, delimited with triple backticks.\n",
        "\n",
        "  ```\n",
        "  {text}\n",
        "  ```\n",
        "  \"\"\"\n",
        "  response = palm.generate_text(\n",
        "      model=model_bison,\n",
        "      prompt=prompt,\n",
        "      temperature=0\n",
        "  )\n",
        "  return response.result"
      ],
      "metadata": {
        "id": "1qrQwd7IvS8c"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "My Name is Aakash Singh. Graduated from IIT Bombay. My Home Town is Siwan\n",
        "Distric of Bihar.\n",
        "I've interest in Data Science and Artificial intelligence field.\n",
        "I've been working as an Associate Data Scientist at Hdfc Life.\n",
        "I'm also uploading project related to GenAI on my Youtube Chaneel.\n",
        "My Youtube channel is IITian Aakash Singh[IITB]\n",
        "\"\"\"\n",
        "print(Generate_summary(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PoblSqSwkHp",
        "outputId": "ac4e1781-cd07-4e31-913f-de262eeb6b14"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aakash Singh is an Associate Data Scientist at HDFC Life. He graduated from IIT Bombay and has an interest in Data Science and Artificial Intelligence. He uploads projects related to GenAI on his YouTube channel, IITian Aakash Singh[IITB].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**gradio APP**"
      ],
      "metadata": {
        "id": "vEa1MrQQxi5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "ycOf8f5uy29r"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio\n",
        "interface = gradio.Interface(fn = Generate_summary,\n",
        "                             inputs=[gradio.Textbox(label=\"Enter Text here...\")],\n",
        "                             outputs = [gradio.Textbox(label=\"Output Summary here... \")],\n",
        "                             title=\"Text Summarizer app\")\n",
        "interface.launch(share=True,debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "-kHqbgfoxQXC",
        "outputId": "29c858a4-253f-43fe-ae26-794df1fa707f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://b2be5484608467b270.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b2be5484608467b270.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://b2be5484608467b270.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interface.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72RVGTSrywtb",
        "outputId": "0766b0a1-7339-4900-c568-59e4782aa6f3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "I'd be happy to explain a project I worked on that involved image classification. The project aimed to build a document classification model using a pre-trained neural network architecture called DenseNet121. The goal was to accurately classify various types of documents such as Aadhar cards, PAN cards, passports, and more.\"\n",
        "Step 1: Importing Libraries\n",
        "The project began by importing the necessary libraries, including NumPy and TensorFlow. TensorFlow is a powerful library for building and training neural networks, while NumPy provides tools for numerical computations. I also imported components from Keras, a high-level neural network API, for using the DenseNet121 pre-trained model\n",
        "Step 2: Setting Constants\n",
        "After importing the libraries, I set up important constants that define aspects of the project. These constants included the data directory path, image dimensions, batch size, number of classes, and the number of epochs for both fine-tuning and the total training process.\n",
        "Step 3: Data Preprocessing and Augmentation\n",
        "Data preprocessing and augmentation are crucial for improving the model's performance. I used an ImageDataGenerator to apply various transformations to the images, such as rotation, shifting, shearing, and more. This helps increase the model's ability to generalize. I also divided the data into training and validation subsets to ensure I could evaluate the model's performance effectively.\"\n",
        "Step 4: Mapping Class Indices to Names\n",
        "In order to keep track of the classes and their respective labels, I mapped class indices to class names using a dictionary. This allowed us to associate human-readable names with the numerical labels and also counted the number of images per class for both the training and validation sets.\n",
        "Step 5: Loading Pre-trained Model\n",
        "To save time and leverage existing knowledge, I used a pre-trained model called DenseNet121. This model has already learned useful features from a large dataset, which I can then fine-tune to our specific classification task. I excluded the fully connected layers at the top of the model to customize it for our own task.\n",
        "Step 6: Fine-tuning by Unfreezing Layers\n",
        "For fine-tuning, I unfroze the last 20 layers of the base model. This allowed us to adapt those layers to the specific characteristics of our document classification problem.\n",
        "Step 7: Adding Custom Layers and Compiling Model\n",
        "I then constructed a new neural network by adding custom layers on top of the pre-trained base model. This included a Global Average Pooling layer to reduce dimensions, a dense layer for classification, and a dropout layer to prevent overfitting. After setting up the architecture, I compiled the model with a low learning rate optimizer to fine-tune it for our task\n",
        "Step 8: Model Training\n",
        "With the model architecture and compilation ready, I moved on to training the model. I used the fit method to train the model on the training data and validated its performance using the validation data. I trained for the specified number of epochs.\n",
        "Step 9: Model Evaluation\n",
        "After training, I evaluated the model's performance on the validation data. I generated predictions for the validation dataset and compared them to the true labels. This allowed us to create a classification report with metrics such as precision, recall, and F1-score. I also calculated a confusion matrix to understand how well the model was classifying different types of documents\n",
        "Step 10: Model Saving\n",
        "Finally, after achieving a satisfactory level of performance, I saved the trained model to a file. This allowed us to reuse the model later without having to retrain it from scratch.\n",
        "In conclusion, this project showcased how to leverage pre-trained neural network architectures, perform fine-tuning, and effectively build and train an image classification model. It's a great example of how machine learning can automate tasks like document sorting or verification, saving time and reducing errors\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Myut0iytz1Ii"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}